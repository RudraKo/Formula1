{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80ce26d",
   "metadata": {},
   "source": [
    "# F1 Data Preparation Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook prepares Formula 1 historical data (1950-2020) for analysis by:\n",
    "1. **Downloading** raw data from Kaggle's F1 dataset\n",
    "2. **Cleaning** and standardizing data types and formats\n",
    "3. **Merging** related tables to create comprehensive datasets\n",
    "4. **Exporting** processed data for downstream analysis\n",
    "\n",
    "## Dataset Information\n",
    "- **Source**: Kaggle - Formula 1 World Championship (1950-2020)\n",
    "- **Coverage**: 70+ years of F1 history\n",
    "- **Tables**: Drivers, Constructors, Races, Results, Lap Times, Pit Stops, Qualifying, Standings\n",
    "\n",
    "## Output\n",
    "- `../data/clean_results.csv` - Race results with driver, constructor, and race metadata\n",
    "- `../data/clean_lap_times.csv` - Individual lap timing data with driver information\n",
    "- `../data/clean_pit_stops.csv` - Pit stop durations and timing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1932d86",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll use:\n",
    "- **kagglehub**: To download the F1 dataset from Kaggle\n",
    "- **pandas**: For data manipulation and cleaning\n",
    "- **numpy**: For numerical operations\n",
    "- **os**: For file path management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6391b68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475a20d",
   "metadata": {},
   "source": [
    "## Step 1: Load Raw Data from Kaggle\n",
    "\n",
    "### What We're Doing:\n",
    "- Download the complete F1 dataset using Kaggle's API\n",
    "- The dataset is automatically cached locally for future runs\n",
    "- We load 8 key tables that contain different aspects of F1 data\n",
    "\n",
    "### Key Tables:\n",
    "- **drivers.csv**: Driver information (name, nationality, DOB)\n",
    "- **constructors.csv**: Team/constructor details\n",
    "- **races.csv**: Race calendar and circuit information\n",
    "- **results.csv**: Final race results and positions\n",
    "- **lap_times.csv**: Individual lap timing data (millions of records)\n",
    "- **pit_stops.csv**: Pit stop timing and duration\n",
    "- **qualifying.csv**: Qualifying session results\n",
    "- **driver_standings.csv**: Championship points after each race\n",
    "\n",
    "### Data Quality Note:\n",
    "The original dataset uses `\\N` to represent missing values, which we handle during import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22024c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Downloading dataset...\")\n",
    "    # This will use the cached path if already downloaded\n",
    "    path = kagglehub.dataset_download(\"rohanrao/formula-1-world-championship-1950-2020\")\n",
    "    print(f\"Path to dataset files: {path}\")\n",
    "\n",
    "    files = [f for f in os.listdir(path) if f.endswith('.csv')]\n",
    "    \n",
    "    data = {}\n",
    "    # Key tables we need\n",
    "    key_files = {\n",
    "        'drivers': 'drivers.csv',\n",
    "        'constructors': 'constructors.csv',\n",
    "        'races': 'races.csv',\n",
    "        'results': 'results.csv', \n",
    "        'lap_times': 'lap_times.csv', \n",
    "        'pit_stops': 'pit_stops.csv', \n",
    "        'qualifying': 'qualifying.csv', \n",
    "        'driver_standings': 'driver_standings.csv'\n",
    "    }\n",
    "\n",
    "    for name, filename in key_files.items():\n",
    "        if filename in files:\n",
    "            file_path = os.path.join(path, filename)\n",
    "            # Handle \\N as NaN explicitly for all files\n",
    "            df = pd.read_csv(file_path, na_values=['\\\\N']) \n",
    "            data[name] = df\n",
    "            print(f\"Loaded {name}: {df.shape}\")\n",
    "        else:\n",
    "            print(f\"WARNING: {filename} not found!\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a17036",
   "metadata": {},
   "source": [
    "### Execute the Download\n",
    "\n",
    "Running this cell will download the dataset (or use cached version if already downloaded) and load all tables into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d4b95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Path to dataset files: /Users/punarvashu/.cache/kagglehub/datasets/rohanrao/formula-1-world-championship-1950-2020/versions/24\n",
      "Loaded drivers: (861, 9)\n",
      "Loaded constructors: (212, 5)\n",
      "Loaded races: (1125, 18)\n",
      "Loaded results: (26759, 18)\n",
      "Loaded lap_times: (589081, 6)\n",
      "Loaded pit_stops: (11371, 7)\n",
      "Loaded qualifying: (10494, 9)\n",
      "Loaded driver_standings: (34863, 7)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601190f",
   "metadata": {},
   "source": [
    "## Step 2: Clean and Standardize Data\n",
    "\n",
    "### Why Clean?\n",
    "Raw data often has inconsistencies that can break analysis:\n",
    "- Mixed data types (strings where numbers should be)\n",
    "- Invalid values (negative lap times, impossible positions)\n",
    "- Redundant information (URLs we don't need)\n",
    "- Inconsistent date formats\n",
    "\n",
    "### Cleaning Operations by Table:\n",
    "\n",
    "#### 1. **Drivers Table**\n",
    "   - Combine first and last names into `driver_name` for easier reference\n",
    "   - Convert date of birth to proper datetime format\n",
    "   - Remove unnecessary URL columns\n",
    "\n",
    "#### 2. **Constructors Table**\n",
    "   - Remove URL columns to reduce data size\n",
    "   - Keep essential team information\n",
    "\n",
    "#### 3. **Races Table**\n",
    "   - Standardize race dates to datetime objects\n",
    "   - Keep race timing information for scheduling analysis\n",
    "\n",
    "#### 4. **Results Table** (Most Critical!)\n",
    "   - Convert numeric columns (grid position, points, lap count) to proper numeric types\n",
    "   - Remove invalid records (positions < 1)\n",
    "   - Handle DNF (Did Not Finish) status codes\n",
    "\n",
    "#### 5. **Lap Times Table**\n",
    "   - Ensure all lap times are positive numbers\n",
    "   - Remove impossible values (0 or negative milliseconds)\n",
    "\n",
    "#### 6. **Pit Stops Table**\n",
    "   - Validate pit stop durations are positive\n",
    "   - Filter out erroneous entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b4bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    print(\"\\n--- Starting Data Cleaning ---\")\n",
    "    \n",
    "    # 1. Clean Drivers\n",
    "    if 'drivers' in data:\n",
    "        df = data['drivers']\n",
    "        # Combine name\n",
    "        df['driver_name'] = df['forename'] + ' ' + df['surname']\n",
    "        # Date of birth\n",
    "        df['dob'] = pd.to_datetime(df['dob'], errors='coerce')\n",
    "        # Drop url\n",
    "        df.drop(columns=['url'], inplace=True, errors='ignore')\n",
    "        data['drivers'] = df\n",
    "        print(\"Cleaned drivers\")\n",
    "\n",
    "    # 2. Clean Constructors\n",
    "    if 'constructors' in data:\n",
    "        df = data['constructors']\n",
    "        df.drop(columns=['url'], inplace=True, errors='ignore')\n",
    "        data['constructors'] = df\n",
    "        print(\"Cleaned constructors\")\n",
    "\n",
    "    # 3. Clean Races\n",
    "    if 'races' in data:\n",
    "        df = data['races']\n",
    "        # Date to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        # Drop url\n",
    "        df.drop(columns=['url'], inplace=True, errors='ignore')\n",
    "        # Filter for relevant columns to keep master table lighter\n",
    "        # We assume 'time' in races is race start time, might be useful, keeping it for now\n",
    "        data['races'] = df\n",
    "        print(\"Cleaned races\")\n",
    "\n",
    "    # 4. Clean Results\n",
    "    if 'results' in data:\n",
    "        df = data['results']\n",
    "        # numeric conversion for critical columns\n",
    "        cols_to_numeric = ['number', 'grid', 'position', 'points', 'laps', 'milliseconds', 'fastestLap', 'rank', 'fastestLapSpeed']\n",
    "        for col in cols_to_numeric:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        \n",
    "        # Sanity Checks\n",
    "        # Filter out rows where positionOrder is < 1 (shouldn't happen but good to check)\n",
    "        original_len = len(df)\n",
    "        df = df[df['positionOrder'] >= 1]\n",
    "        if len(df) < original_len:\n",
    "            print(f\"Removed {original_len - len(df)} invalid positionOrder rows in results\")\n",
    "        \n",
    "        data['results'] = df\n",
    "        print(\"Cleaned results\")\n",
    "\n",
    "    # 5. Clean Lap Times\n",
    "    if 'lap_times' in data:\n",
    "        df = data['lap_times']\n",
    "        df['milliseconds'] = pd.to_numeric(df['milliseconds'], errors='coerce')\n",
    "        # Sanity check: lap time > 0\n",
    "        df = df[df['milliseconds'] > 0]\n",
    "        data['lap_times'] = df\n",
    "        print(\"Cleaned lap_times\")\n",
    "\n",
    "    # 6. Clean Pit Stops\n",
    "    if 'pit_stops' in data:\n",
    "        df = data['pit_stops']\n",
    "        df['milliseconds'] = pd.to_numeric(df['milliseconds'], errors='coerce')\n",
    "        df = df[df['milliseconds'] > 0]\n",
    "        data['pit_stops'] = df\n",
    "        print(\"Cleaned pit_stops\")\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c833596",
   "metadata": {},
   "source": [
    "### Execute Cleaning Pipeline\n",
    "\n",
    "This will systematically clean each table and report progress. Watch for any warnings about removed invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b48c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Cleaning ---\n",
      "Cleaned drivers\n",
      "Cleaned constructors\n",
      "Cleaned races\n",
      "Cleaned results\n",
      "Cleaned lap_times\n",
      "Cleaned pit_stops\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcfff0",
   "metadata": {},
   "source": [
    "## Step 3: Merge Related Tables\n",
    "\n",
    "### Why Merge?\n",
    "The F1 dataset follows a relational database design with separate tables linked by IDs. For analysis, we need comprehensive records that combine information from multiple tables.\n",
    "\n",
    "### Merging Strategy:\n",
    "\n",
    "#### **Results Master Table**\n",
    "Combines race results with contextual information:\n",
    "- **Base**: `results.csv` (race outcomes)\n",
    "- **+ races**: Adds year, circuit, race name, date\n",
    "- **+ drivers**: Adds driver name, nationality, code\n",
    "- **+ constructors**: Adds team name, team nationality\n",
    "\n",
    "**Use Case**: Analyzing driver performance, team comparisons, historical trends\n",
    "\n",
    "#### **Lap Times Master Table**\n",
    "Enriches lap-by-lap timing with identifiable information:\n",
    "- **Base**: `lap_times.csv` (individual lap times)\n",
    "- **+ races**: Adds race context (year, circuit)\n",
    "- **+ drivers**: Adds driver names for readability\n",
    "\n",
    "**Use Case**: Pace analysis, tire degradation, strategy evaluation\n",
    "\n",
    "#### **Pit Stops Master Table**\n",
    "Combines pit stop data with race and driver details:\n",
    "- **Base**: `pit_stops.csv` (pit stop timing)\n",
    "- **+ races**: Adds race context\n",
    "- **+ drivers**: Adds driver identification\n",
    "\n",
    "**Use Case**: Pit crew performance, strategy analysis\n",
    "\n",
    "### Join Type\n",
    "We use LEFT joins to preserve all records from the base table, even if some metadata is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa320f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(data):\n",
    "    print(\"\\n--- Starting Data Merging ---\")\n",
    "    \n",
    "    # Prerequisite check\n",
    "    required = ['results', 'races', 'drivers', 'constructors', 'lap_times', 'pit_stops']\n",
    "    for req in required:\n",
    "        if req not in data:\n",
    "            print(f\"CRITICAL: Missing {req} table for merging.\")\n",
    "            return {}\n",
    "\n",
    "    races = data['races'][['raceId', 'year', 'round', 'circuitId', 'name', 'date']]\n",
    "    races = races.rename(columns={'name': 'race_name', 'date': 'race_date'})\n",
    "    \n",
    "    drivers = data['drivers'][['driverId', 'driver_name', 'nationality', 'code']]\n",
    "    drivers = drivers.rename(columns={'nationality': 'driver_nationality'})\n",
    "    \n",
    "    constructors = data['constructors'][['constructorId', 'name', 'nationality']]\n",
    "    constructors = constructors.rename(columns={'name': 'constructor_name', 'nationality': 'constructor_nationality'})\n",
    "\n",
    "    # 1. Race Results Master\n",
    "    print(\"Merging Results Master...\")\n",
    "    results = data['results']\n",
    "    \n",
    "    # Merge with Races\n",
    "    res_master = pd.merge(results, races, on='raceId', how='left')\n",
    "    \n",
    "    # Merge with Drivers\n",
    "    res_master = pd.merge(res_master, drivers, on='driverId', how='left')\n",
    "    \n",
    "    # Merge with Constructors\n",
    "    res_master = pd.merge(res_master, constructors, on='constructorId', how='left')\n",
    "    \n",
    "    # 2. Lap Times Master\n",
    "    print(\"Merging Lap Times Master...\")\n",
    "    laps = data['lap_times']\n",
    "    \n",
    "    # Merge with Races\n",
    "    laps_master = pd.merge(laps, races, on='raceId', how='left')\n",
    "    \n",
    "    # Merge with Drivers\n",
    "    laps_master = pd.merge(laps_master, drivers, on='driverId', how='left')\n",
    "    \n",
    "    # 3. Pit Stops Master\n",
    "    print(\"Merging Pit Stops Master...\")\n",
    "    pits = data['pit_stops']\n",
    "    \n",
    "    # Merge with Races\n",
    "    pits_master = pd.merge(pits, races, on='raceId', how='left')\n",
    "    \n",
    "    # Merge with Drivers\n",
    "    pits_master = pd.merge(pits_master, drivers, on='driverId', how='left')\n",
    "    \n",
    "    return {\n",
    "        'results_master': res_master,\n",
    "        'lap_times_master': laps_master,\n",
    "        'pit_stops_master': pits_master\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bcfbf",
   "metadata": {},
   "source": [
    "### Execute Merging Operations\n",
    "\n",
    "This will create three comprehensive master tables by joining related information. The process may take a moment for large tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32dfadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Merging ---\n",
      "Merging Results Master...\n",
      "Merging Lap Times Master...\n",
      "Merging Pit Stops Master...\n"
     ]
    }
   ],
   "source": [
    "# Merge the data\n",
    "merged = merge_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db53f93",
   "metadata": {},
   "source": [
    "## Step 4: Export Clean Data\n",
    "\n",
    "### Output Location\n",
    "All cleaned data is saved to `../data/` folder for:\n",
    "- Easy access by analysis notebooks\n",
    "- Version control (if needed)\n",
    "- Reproducible analysis pipeline\n",
    "\n",
    "### Output Files:\n",
    "1. **clean_results.csv** - Complete race results (~26,000 rows)\n",
    "2. **clean_lap_times.csv** - Lap-by-lap timing (~500,000+ rows)\n",
    "3. **clean_pit_stops.csv** - Pit stop records (~9,000+ rows)\n",
    "\n",
    "### File Format\n",
    "CSV (Comma-Separated Values) for:\n",
    "- Universal compatibility\n",
    "- Easy inspection in spreadsheet software\n",
    "- Fast loading in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f93c19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(merged_data):\n",
    "    print(\"\\n--- Exporting Data ---\")\n",
    "    for name, df in merged_data.items():\n",
    "        filename = f\"clean_{name.replace('_master', '')}.csv\"\n",
    "        # We keep the _master suffix logic or just simplify?\n",
    "        # User requested: clean_results.csv, clean_lap_times.csv, clean_pit_stops.csv\n",
    "        # The keys are results_master, lap_times_master...\n",
    "        # Let's map strict names\n",
    "        if name == 'results_master':\n",
    "            filename = 'clean_results.csv'\n",
    "        elif name == 'lap_times_master':\n",
    "            filename = 'clean_lap_times.csv'\n",
    "        elif name == 'pit_stops_master':\n",
    "            filename = 'clean_pit_stops.csv'\n",
    "            \n",
    "        print(f\"Saving {filename} (Shape: {df.shape})...\")\n",
    "        df.to_csv(f\"../data/{filename}\", index=False)\n",
    "        print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5332a7ad",
   "metadata": {},
   "source": [
    "### Execute Data Export\n",
    "\n",
    "Save all processed datasets to the data folder. These files will be used by all analysis notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad610a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Exporting Data ---\n",
      "Saving clean_results.csv (Shape: (26759, 28))...\n",
      "Saved.\n",
      "Saving clean_lap_times.csv (Shape: (589081, 14))...\n",
      "Saved.\n",
      "Saving clean_pit_stops.csv (Shape: (11371, 15))...\n",
      "Saved.\n",
      "\n",
      "Pipeline Complete!\n"
     ]
    }
   ],
   "source": [
    "# Export the merged data\n",
    "if merged:\n",
    "    export_data(merged)\n",
    "    print(\"\\nPipeline Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39faadcc",
   "metadata": {},
   "source": [
    "## Data Preparation Complete!\n",
    "\n",
    "### What We Achieved:\n",
    " Downloaded complete F1 dataset (1950-2020)  \n",
    " Cleaned and standardized all tables  \n",
    " Merged related information into comprehensive datasets  \n",
    " Exported analysis-ready CSV files  \n",
    "\n",
    "### Output Files Created:\n",
    "Located in `../data/` folder:\n",
    "1. **clean_results.csv** (~26,000 rows)\n",
    "   - Race results with driver, team, and race details\n",
    "   \n",
    "2. **clean_lap_times.csv** (~500,000+ rows)\n",
    "   - Individual lap times with driver and race context\n",
    "   \n",
    "3. **clean_pit_stops.csv** (~9,000+ rows)\n",
    "   - Pit stop durations with driver and race information\n",
    "\n",
    "### Data Quality Checks Performed:\n",
    "-  Removed invalid position records\n",
    "-  Filtered impossible lap times (≤0ms)\n",
    "-  Converted dates to proper datetime format\n",
    "-  Standardized numeric columns\n",
    "-  Handled missing values (\\\\N → NaN)\n",
    "\n",
    "### Ready for Analysis!\n",
    "These clean datasets are now ready to use in:\n",
    "- `driver_analytics.ipynb` - Driver performance analysis\n",
    "- `strategy_analytics.ipynb` - Race strategy and championship analysis\n",
    "- Custom analysis notebooks you create\n",
    "\n",
    "### Data Coverage:\n",
    "- **70+ years** of F1 history\n",
    "- **~850 races** across all eras\n",
    "- **~850 drivers** who competed\n",
    "- **Millions** of individual lap times\n",
    "\n",
    "### Next Steps:\n",
    "1. Open `driver_analytics.ipynb` to analyze driver performance\n",
    "2. Open `strategy_analytics.ipynb` to analyze race strategy\n",
    "3. Use the clean data for your own custom analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
